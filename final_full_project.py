# -*- coding: utf-8 -*-
"""final full project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CgcGw4qgt_6DJsXwY1jxOOmGkwoZ6VtY
"""

!pip install pandas numpy matplotlib seaborn scikit-learn nltk wordcloud tensorflow

"""**Upload File**"""

import pandas as pd

# Upload a CSV file
from google.colab import files
uploaded = files.upload()

# Read the dataset
AR = pd.read_csv('Airline_review_assignment_data.csv')


# Display the first few rows
print("First few rows of the dataset:")
print(AR.head())

"""**VISUALIZE MISSING VALUES**"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Define categorical and continuous variables
categorical_vars = ['Airline Name', 'Verified', 'Type Of Traveller', 'Seat Type', 'Recommended']
continuous_vars = ['Overall_Rating', 'Seat Comfort', 'Cabin Staff Service', 'Food & Beverages',
                   'Ground Service', 'Inflight Entertainment', 'Wifi & Connectivity', 'Value For Money']

# Plot bar charts for categorical variables
plt.figure(figsize=(16, 10))
for i, var in enumerate(categorical_vars, 1):
    plt.subplot(2, 3, i)  # Adjusted subplot layout
    ax = sns.countplot(data=AR, x=var)
    plt.title(f'Bar Chart for {var}')

    # Add annotations for missing values
    missing_count = AR[var].isnull().sum()
    plt.text(0.5, 0.9, f'Missing: {missing_count}', fontsize=10, color='red', ha='center', transform=plt.gca().transAxes)

    # Place values inside the bars
    for p in ax.patches:
        if not np.isnan(p.get_height()):
            ax.annotate(f'{int(p.get_height())}',
                        (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha='center', va='center', fontsize=8, color='black',
                        xytext=(0, 5), textcoords='offset points')

plt.tight_layout()
plt.show()

# Plot violin plots for continuous variables
plt.figure(figsize=(16, 10))
for i, var in enumerate(continuous_vars, 1):
    plt.subplot(2, 4, i)  # Adjusted subplot layout
    ax = sns.violinplot(data=AR, y=var)
    plt.title(f'Violin Plot for {var}')

    # Add annotations for missing values
    missing_count = AR[var].isnull().sum()
    plt.text(0.5, 0.9, f'Missing: {missing_count}', fontsize=10, color='red', ha='center', transform=plt.gca().transAxes)

plt.tight_layout()
plt.show()

#Convert the data type of 'Overall_Rating' column to integer
AR['Overall_Rating'] = pd.to_numeric(AR['Overall_Rating'], errors='coerce').fillna(0).astype(float)

# Print the data type of 'Overall_Rating' column after conversion
print("Data type of 'Overall_Rating' after conversion:", AR['Overall_Rating'].dtype)

# Display the first 5 rows of the dataframe to verify the change
print("\nFirst 5 rows of the dataframe:")
print(AR.head())

# Replace 'n' with a blank ('')
AR['Overall_Rating'] = AR['Overall_Rating'].replace('n', '')

print(AR.dtypes)

"""**Handling Missing Values **"""

import pandas as pd

# Replace blank values ('') with NaN for proper handling
AR.replace('', pd.NA, inplace=True)

# Identify categorical and numerical columns
categorical_cols = AR.select_dtypes(include=['object', 'bool']).columns
numerical_cols = AR.select_dtypes(include=['number']).columns

# Fill categorical columns with 'Unknown'
AR[categorical_cols] = AR[categorical_cols].fillna('Unknown')

# Replace 0s in numerical columns with NaN (if 0 is not a valid value)
AR[numerical_cols] = AR[numerical_cols].replace(0, pd.NA)

# For the numerical columns with missing values, fill missing values with the mode (most frequent value) for each airline
for col in numerical_cols:
    AR[col] = AR.groupby('Airline Name')[col].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else pd.NA))

# Ensure no decimals in numerical columns (convert to integer)
AR[numerical_cols] = AR[numerical_cols].round(0).astype('Int64')  # Convert to integer without decimals

# Display the number of missing values for each column after cleaning
print(AR.isna().sum())

# Display the first few rows of the cleaned data
print(AR.head())

# Replace 0s in numerical columns with NaN (so they don't appear unnecessarily)
AR[numerical_cols] = AR[numerical_cols].replace(0, pd.NA)

# Convert each numerical column separately to float and fill missing values with median
for col in numerical_cols:
    AR[col] = AR[col].astype(float)  # Explicitly convert to float
    AR[col] = AR[col].fillna(AR[col].median())  # Fill NaN with median value

# Display the number of missing values for each column after cleaning
print("Missing values in each column after handling:")
print(AR.isna().sum())

# Display the first few rows of the cleaned data
print(AR.head())

boolean_cols = AR.select_dtypes(include=['bool']).columns

# For boolean columns, replace 0 or False with 'Unknown' and handle empty rows
for col in boolean_cols:
    AR[col] = AR[col].fillna('Unknown')  # Replace NaN with 'Unknown'
    AR[col] = AR[col].replace({False: 'Unknown', 0: 'Unknown'})

# Count how many '0' values are in each column
zero_counts = (AR == 0).sum()
print("Zero counts per column:")
print(zero_counts)

# Count how many empty (blank) values are in each column
empty_counts = AR.isin(['', None]).sum()
print("\nEmpty (blank) values counts per column:")
print(empty_counts)

# Data types and summary statistics
print("\nData types and summary statistics:")
print(AR.info())
print(AR.describe())

"""**Document the cleaning process**"""

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from collections import Counter
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import string

# Download required NLTK data
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('punkt_tab',quiet=True)

"""**Descriptive Staistics **"""

# Data Cleaning and Preparation
# Handle Missing Values
AR = AR.dropna(subset=['Review'])

# Text Lowercasing
AR['Review'] = AR['Review'].str.lower()

# Remove HTML Tags
def remove_html_tags(text):
    clean = re.compile('<.*?>')
    return clean.sub(r'', text)

AR['Review'] = AR['Review'].apply(remove_html_tags)

# Remove URLs
def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

AR['Review'] = AR['Review'].apply(remove_urls)

# Remove Punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

AR['Review'] = AR['Review'].apply(remove_punctuation)

# Remove Special Characters and Numbers
def remove_special_characters(text):
    pattern = r'[^a-zA-Z\s]'
    return re.sub(pattern, '', text)

AR['Review'] = AR['Review'].apply(remove_special_characters)

# Tokenization
def tokenize_text(text):
    return word_tokenize(text)

AR['Review'] = AR['Review'].apply(tokenize_text)

# Remove Stop Words
stop_words = set(stopwords.words('english'))
def remove_stopwords(tokens):
    return [token for token in tokens if token not in stop_words]

AR['Review'] = AR['Review'].apply(remove_stopwords)

# Lemmatization
lemmatizer = WordNetLemmatizer()
def lemmatize_text(tokens):
    return [lemmatizer.lemmatize(token) for token in tokens]

AR['Review'] = AR['Review'].apply(lemmatize_text)

# Join Tokens Back
def join_tokens(tokens):
    return ' '.join(tokens)

AR['Review'] = AR['Review'].apply(join_tokens)

# Initial Findings from the Cleaned Dataset
# Most Frequent Words
all_words = ' '.join(AR['Review']).split()
word_counts = Counter(all_words)
most_common_words = word_counts.most_common(20)
print("Most Common Words:", most_common_words)

"""**Descriptive and Predictive Analytics (Numerical features)**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


# Select numeric columns
numeric_columns = ['Overall_Rating', 'Seat Comfort', 'Cabin Staff Service',
                      'Food & Beverages', 'Ground Service', 'Inflight Entertainment',
                      'Wifi & Connectivity', 'Value For Money']
# Compute descriptive statistics
desc_stats = AR[numeric_columns].describe()
print("Descriptive Statistics:")
print(desc_stats)

## Visualization
print("\n### Visualization of Key Features")

# 1. Distribution of Overall Ratings
plt.figure(figsize=(10, 6))
sns.histplot(AR['Overall_Rating'], bins=10, kde=True)
plt.title('Distribution of Overall Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

# 2. Seat Comfort vs. Overall Rating
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Seat Comfort', y='Overall_Rating', data=AR, alpha=0.5)
plt.title('Seat Comfort vs. Overall Rating')
plt.xlabel('Seat Comfort Rating')
plt.ylabel('Overall Rating')
plt.show()

# 3. Overall Ratings by Traveller Type
plt.figure(figsize=(12, 6))
sns.boxplot(x='Type Of Traveller', y='Overall_Rating', data=AR)
plt.title('Overall Ratings by Traveller Type')
plt.xticks(rotation=45)
plt.show()

#4. Distribution of 'Value For Money' Ratings
plt.figure(figsize=(8, 6))
sns.histplot(AR['Value For Money'], kde=True)
plt.title('Distribution of Value For Money Ratings')
plt.xlabel('Value For Money Rating')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

#5.  Comparison:  Boxplot of Overall Rating vs. Recommended
plt.figure(figsize=(8, 6))
sns.boxplot(x='Recommended', y='Overall_Rating', data=AR)
plt.title('Overall Rating vs. Recommended')
plt.xlabel('Recommended (0=No, 1=Yes)')
plt.ylabel('Overall Rating')
plt.tight_layout()
plt.show()

#6. Create the countplot
plt.figure(figsize=(8, 6))  # Adjust figure size for better readability
sns.countplot(x='Food & Beverages', hue='Recommended', data=AR)
plt.title('Food & Beverages vs. Recommendation')
plt.xlabel('Food & Beverages')
plt.ylabel('Number of Reviews')
plt.legend(title='Recommended', labels=['No', 'Yes']) # Customize legend
plt.show()

#7. Create the scatter plot
plt.figure(figsize=(10, 6))  # Adjust figure size as needed
sns.scatterplot(x='Cabin Staff Service', y='Overall_Rating', hue='Recommended', data=AR, alpha=0.7)
plt.title('Cabin Staff Service vs. Overall Rating')
plt.xlabel('Cabin Staff Service Rating')
plt.ylabel('Overall Rating')
plt.legend(title='Recommended', labels=['No', 'Yes'])
plt.grid(True)  # Add gridlines for better readability
plt.show()

"""Feature Selection"""

# Select only numerical columns
numerical_cols = AR.select_dtypes(include=['number']).columns

# Compute the correlation matrix
corr_matrix = AR[numerical_cols].corr()

# Display the correlation matrix
print("Correlation Matrix:")
print(corr_matrix)

# Plot the correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix Heatmap")
plt.show()

"""**Machine learning With numerical Features**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, confusion_matrix
#-------------------------------------------------------model 1 ---------------------------------------------------------------------------------------------
# Define the model columns based on correlation analysis
model_columns = ['Ground Service', 'Inflight Entertainment',
'Wifi & Connectivity', 'Value For Money']

# Ensure numeric data in selected columns
AR[model_columns] = AR[model_columns].apply(pd.to_numeric, errors='coerce')

# Check if there are any rows with missing values in the selected columns
missing_data = AR[model_columns].isnull().sum()
if missing_data.any():
    print(f"Columns with missing values:\n{missing_data}")

# Drop rows with missing values in selected columns and the target column 'Recommended'
AR_cleaned = AR.dropna(subset=model_columns + ['Recommended'])

# Check if the dataset is empty after cleaning
if AR_cleaned.empty:
    raise ValueError("The dataset is empty after preprocessing. Check the input data and column selections.")

# Map the target variable ('Recommended') to binary values
AR_cleaned['Recommended'] = AR_cleaned['Recommended'].map({'yes': 1, 'no': 0})

# Prepare the input (X) and output (y)
X = AR_cleaned[model_columns]
y = AR_cleaned['Recommended']

# Check if the dataset is empty after processing
if X.empty or y.empty:
    raise ValueError("The dataset is empty after preprocessing. Check the input data and column selections.")

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define an evaluation function
def evaluate_model(y_true, y_pred, model_name):
    print(f"{model_name} Performance:")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred):.4f}")
    print(f"Recall: {recall_score(y_true, y_pred):.4f}")
    print(f"F1-score: {f1_score(y_true, y_pred):.4f}\n")

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.show()

# Logistic Regression Model
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
evaluate_model(y_test, lr_pred, "Logistic Regression")

# Random Forest Model
rf_model = RandomForestClassifier(random_state=42, n_estimators=100)  # Using 100 trees
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
evaluate_model(y_test, rf_pred, "Random Forest")

# Hyperparameter Tuning for Logistic Regression
lr_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}
lr_grid = GridSearchCV(LogisticRegression(random_state=42), lr_param_grid, cv=5)
lr_grid.fit(X_train, y_train)
print(f"Best C for Logistic Regression: {lr_grid.best_params_['C']}")

# Hyperparameter Tuning for Random Forest
rf_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5)
rf_grid.fit(X_train, y_train)
print(f"Best Parameters for Random Forest: {rf_grid.best_params_}")

# Evaluate tuned models
lr_tuned_pred = lr_grid.best_estimator_.predict(X_test)
rf_tuned_pred = rf_grid.best_estimator_.predict(X_test)

evaluate_model(y_test, lr_tuned_pred, "Tuned Logistic Regression")
evaluate_model(y_test, rf_tuned_pred, "Tuned Random Forest")

# ROC Curve for Logistic Regression (Basic and Tuned)
lr_probs = lr_model.predict_proba(X_test)[:, 1]
lr_tuned_probs = lr_grid.best_estimator_.predict_proba(X_test)[:, 1]

fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probs)
fpr_lr_tuned, tpr_lr_tuned, _ = roc_curve(y_test, lr_tuned_probs)

roc_auc_lr = roc_auc_score(y_test, lr_probs)
roc_auc_lr_tuned = roc_auc_score(y_test, lr_tuned_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, color='blue', lw=2, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')
plt.plot(fpr_lr_tuned, tpr_lr_tuned, color='green', lw=2, label=f'Tuned Logistic Regression (AUC = {roc_auc_lr_tuned:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend(loc="lower right")
plt.show()

# ROC Curve for Random Forest (Basic and Tuned)
rf_probs = rf_model.predict_proba(X_test)[:, 1]
rf_tuned_probs = rf_grid.best_estimator_.predict_proba(X_test)[:, 1]

fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)
fpr_rf_tuned, tpr_rf_tuned, _ = roc_curve(y_test, rf_tuned_probs)

roc_auc_rf = roc_auc_score(y_test, rf_probs)
roc_auc_rf_tuned = roc_auc_score(y_test, rf_tuned_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')
plt.plot(fpr_rf_tuned, tpr_rf_tuned, color='green', lw=2, label=f'Tuned Random Forest (AUC = {roc_auc_rf_tuned:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest')
plt.legend(loc="lower right")
plt.show()

# ROC Curve for Basic Logistic Regression
lr_probs = lr_model.predict_proba(X_test)[:, 1]
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probs)
roc_auc_lr = roc_auc_score(y_test, lr_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, color='blue', lw=2, label=f'AUC = {roc_auc_lr:.2f}')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression (Basic)')
plt.legend(loc="lower right")
plt.show()

# ROC Curve for Tuned Logistic Regression
lr_tuned_probs = lr_grid.best_estimator_.predict_proba(X_test)[:, 1]
fpr_lr_tuned, tpr_lr_tuned, _ = roc_curve(y_test, lr_tuned_probs)
roc_auc_lr_tuned = roc_auc_score(y_test, lr_tuned_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr_lr_tuned, tpr_lr_tuned, color='green', lw=2, label=f'AUC = {roc_auc_lr_tuned:.2f}')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Tuned Logistic Regression')
plt.legend(loc="lower right")
plt.show()

# ROC Curve for Basic Random Forest
rf_probs = rf_model.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)
roc_auc_rf = roc_auc_score(y_test, rf_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label=f'AUC = {roc_auc_rf:.2f}')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest (Basic)')
plt.legend(loc="lower right")
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, confusion_matrix
#-----------------------------------------------------------------Model 2 ----------------------------------------------------------------------
# Define the model columns based on correlation analysis
model_columns = [
'Seat Comfort',
'Wifi & Connectivity',
'Value For Money']

# Ensure numeric data in selected columns
AR[model_columns] = AR[model_columns].apply(pd.to_numeric, errors='coerce')

# Check if there are any rows with missing values in the selected columns
missing_data = AR[model_columns].isnull().sum()
if missing_data.any():
    print(f"Columns with missing values:\n{missing_data}")

# Drop rows with missing values in selected columns and the target column 'Recommended'
AR_cleaned = AR.dropna(subset=model_columns + ['Recommended'])

# Check if the dataset is empty after cleaning
if AR_cleaned.empty:
    raise ValueError("The dataset is empty after preprocessing. Check the input data and column selections.")

# Map the target variable ('Recommended') to binary values
AR_cleaned['Recommended'] = AR_cleaned['Recommended'].map({'yes': 1, 'no': 0})

# Prepare the input (X) and output (y)
X = AR_cleaned[model_columns]
y = AR_cleaned['Recommended']

# Check if the dataset is empty after processing
if X.empty or y.empty:
    raise ValueError("The dataset is empty after preprocessing. Check the input data and column selections.")

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define an evaluation function
def evaluate_model(y_true, y_pred, model_name):
    print(f"{model_name} Performance:")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred):.4f}")
    print(f"Recall: {recall_score(y_true, y_pred):.4f}")
    print(f"F1-score: {f1_score(y_true, y_pred):.4f}\n")

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.show()

# Logistic Regression Model
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
evaluate_model(y_test, lr_pred, "Logistic Regression")

# Random Forest Model
rf_model = RandomForestClassifier(random_state=42, n_estimators=100)  # Using 100 trees
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
evaluate_model(y_test, rf_pred, "Random Forest")

# Hyperparameter Tuning for Logistic Regression
lr_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}
lr_grid = GridSearchCV(LogisticRegression(random_state=42), lr_param_grid, cv=5)
lr_grid.fit(X_train, y_train)
print(f"Best C for Logistic Regression: {lr_grid.best_params_['C']}")

# Hyperparameter Tuning for Random Forest
rf_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5)
rf_grid.fit(X_train, y_train)
print(f"Best Parameters for Random Forest: {rf_grid.best_params_}")

# Evaluate tuned models
lr_tuned_pred = lr_grid.best_estimator_.predict(X_test)
rf_tuned_pred = rf_grid.best_estimator_.predict(X_test)

evaluate_model(y_test, lr_tuned_pred, "Tuned Logistic Regression")
evaluate_model(y_test, rf_tuned_pred, "Tuned Random Forest")

# ROC Curve for Logistic Regression (Basic and Tuned)
lr_probs = lr_model.predict_proba(X_test)[:, 1]
lr_tuned_probs = lr_grid.best_estimator_.predict_proba(X_test)[:, 1]

fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probs)
fpr_lr_tuned, tpr_lr_tuned, _ = roc_curve(y_test, lr_tuned_probs)

roc_auc_lr = roc_auc_score(y_test, lr_probs)
roc_auc_lr_tuned = roc_auc_score(y_test, lr_tuned_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, color='blue', lw=2, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')
plt.plot(fpr_lr_tuned, tpr_lr_tuned, color='green', lw=2, label=f'Tuned Logistic Regression (AUC = {roc_auc_lr_tuned:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend(loc="lower right")
plt.show()

# ROC Curve for Random Forest (Basic and Tuned)
rf_probs = rf_model.predict_proba(X_test)[:, 1]
rf_tuned_probs = rf_grid.best_estimator_.predict_proba(X_test)[:, 1]

fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)
fpr_rf_tuned, tpr_rf_tuned, _ = roc_curve(y_test, rf_tuned_probs)

roc_auc_rf = roc_auc_score(y_test, rf_probs)
roc_auc_rf_tuned = roc_auc_score(y_test, rf_tuned_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')
plt.plot(fpr_rf_tuned, tpr_rf_tuned, color='green', lw=2, label=f'Tuned Random Forest (AUC = {roc_auc_rf_tuned:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest')
plt.legend(loc="lower right")
plt.show()

# ROC Curve for Basic Logistic Regression
lr_probs = lr_model.predict_proba(X_test)[:, 1]
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probs)
roc_auc_lr = roc_auc_score(y_test, lr_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, color='blue', lw=2, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Basic Logistic Regression')
plt.legend(loc="lower right")
plt.show()

# ROC Curve for Basic Random Forest
rf_probs = rf_model.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)
roc_auc_rf = roc_auc_score(y_test, rf_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Basic Random Forest')
plt.legend(loc="lower right")
plt.show()

"""**Descriptive and Predictive Analytics (Text features)**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from textblob import TextBlob  # For sentiment analysis
from sklearn.preprocessing import LabelEncoder



# Sentiment Analysis using TextBlob
def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

AR['Sentiment_Score'] = AR['Review'].apply(get_sentiment)


def categorize_sentiment(score):
    if score > 0.1:  # Slightly positive threshold
        return 'Positive'
    elif score < -0.1:  # Slightly negative threshold
        return 'Negative'
    else:
        return 'Neutral'

AR['Sentiment'] = AR['Sentiment_Score'].apply(categorize_sentiment)


# Descriptive Statistics for Sentiment
print("Sentiment Value Counts:")
print(AR['Sentiment'].value_counts())

print("\nSentiment Score Statistics:")
print(AR['Sentiment_Score'].describe())


# Visualizations

# 1. Sentiment Distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='Sentiment', data=AR, order=['Positive', 'Neutral', 'Negative'])
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Reviews')
plt.show()


# 2. Average Overall Rating by Sentiment
plt.figure(figsize=(8, 6))
sns.barplot(x='Sentiment', y='Overall_Rating', data=AR, order=['Positive', 'Neutral', 'Negative'])
plt.title('Average Overall Rating by Sentiment')
plt.xlabel('Sentiment')
plt.ylabel('Average Overall Rating')
plt.show()
# Insight: Positive sentiment = higher ratings

# 3. Boxplot of Seat Comfort vs. Sentiment
plt.figure(figsize=(8, 6))
sns.boxplot(x='Sentiment', y='Seat Comfort', data=AR, order=['Positive', 'Neutral', 'Negative'])
plt.title('Seat Comfort vs. Sentiment')
plt.xlabel('Sentiment')
plt.ylabel('Seat Comfort Rating')
plt.show()
# Insight: Check if seat comfort ratings differ significantly by sentiment.

# 4. Sentiment vs. Recommended (Countplot)
plt.figure(figsize=(8, 6))
sns.countplot(x='Sentiment', hue='Recommended', data=AR, order=['Positive', 'Neutral', 'Negative'])
plt.title('Sentiment vs. Recommended')
plt.xlabel('Sentiment')
plt.ylabel('Number of Reviews')
plt.legend(title='Recommended (0=No, 1=Yes)')
plt.show()

#5. Stacked Bar Chart of Sentiment by Traveler Type
# Cross-tabulate Sentiment and Type of Traveller
sentiment_by_traveler = pd.crosstab(AR['Type Of Traveller'], AR['Sentiment'])

# Plotting the stacked bar chart
sentiment_by_traveler.plot(kind='bar', stacked=True, figsize=(10, 6))
plt.title('Sentiment by Type of Traveller')
plt.xlabel('Type of Traveller')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 6. Sentiment Trends Over Time
# First, convert 'Review Date' to datetime objects
AR['Review Date'] = pd.to_datetime(AR['Review Date'], format='%dth %B %Y', errors='coerce')

# Extract year and month
AR['YearMonth'] = AR['Review Date'].dt.to_period('M')

# Group by YearMonth and Sentiment, then count the occurrences
sentiment_over_time = AR.groupby(['YearMonth', 'Sentiment']).size().unstack(fill_value=0)

# Plotting the trends
plt.figure(figsize=(12, 6))
sentiment_over_time.plot(kind='line', marker='o')
plt.title('Sentiment Trends Over Time')
plt.xlabel('Year-Month')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()



# 7. Distribution of Sentiment Scores
plt.figure(figsize=(8, 6))
sns.histplot(AR['Sentiment_Score'], kde=True)
plt.title('Distribution of Sentiment Scores')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()

"""**machine Learning with Text features**"""

# Machine Learning Models
# Prepare data for machine learning
X = AR['Review']  # Using the text reviews as input
y = AR['Recommended']

# Text vectorization using TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')  # Limit features, remove stop words
X = vectorizer.fit_transform(X).toarray()  # Convert to array


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model 1: Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)  # Tuned
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

# Model 2: Logistic Regression
lr_model = LogisticRegression(random_state=42, solver='liblinear', C=0.1)  # Tuned
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)

# Model Evaluation
print("\nRandom Forest Performance:")
print("Accuracy:", accuracy_score(y_test, rf_predictions))
print(classification_report(y_test, rf_predictions))
print("Confusion Matrix:\n", confusion_matrix(y_test, rf_predictions))

print("\nLogistic Regression Performance:")
print("Accuracy:", accuracy_score(y_test, lr_predictions))
print(classification_report(y_test, lr_predictions))
print("Confusion Matrix:\n", confusion_matrix(y_test, lr_predictions))


# Feature Importance (for Random Forest - using TF-IDF feature names)
feature_importances = rf_model.feature_importances_
feature_names = vectorizer.get_feature_names_out()  # Get feature names from TF-IDF
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False).head(10)  # Top 10
print("\nTop 10 Important Features (Words):")
print(feature_importance_df)

"""hybrid model"""

# Check if there are any missing values in the 'Recommended' column
missing_recommended = AR['Recommended'].isnull().sum()
print(f"Missing values in 'Recommended' column: {missing_recommended}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, confusion_matrix, classification_report,
                             roc_curve, roc_auc_score)
from sklearn.preprocessing import StandardScaler
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

# Assuming AR is the cleaned dataframe with 'Review' (text) and 'Recommended' (target)

# Define the text and numerical features
# Text Feature: 'Review'
X_text = AR['Review']

# Numerical Features: Select the relevant columns
model_columns = ['Ground Service' , 'Wifi & Connectivity', 'Value For Money']
X_num = AR[model_columns]

# --- TEXT FEATURE PREPROCESSING --- #
# Use TF-IDF Vectorizer to convert text reviews into numerical form
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X_text_vectorized = vectorizer.fit_transform(X_text)

# --- NUMERICAL FEATURE PREPROCESSING --- #
# Standardize numerical features
scaler = StandardScaler()
X_num_scaled = scaler.fit_transform(X_num)

# Combine the text features and numerical features
X_combined = hstack([X_text_vectorized, X_num_scaled])

# Encode the target variable 'Recommended' (from string to numeric internally)
# We will leave the column in string format, but internally the model will map it
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(AR['Recommended'])

# Split the data into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42, stratify=y)

# --- MODELS --- #
# Logistic Regression Model
lr_model = LogisticRegression(random_state=42, solver='liblinear', C=0.1)
lr_model.fit(X_train, y_train)
lr_preds = lr_model.predict(X_test)

# Random Forest Model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# --- EVALUATION --- #
# Function to evaluate the model performance
def evaluate_model(y_true, y_pred, model_name):
    print(f"{model_name} Performance:")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred):.4f}")
    print(f"Recall: {recall_score(y_true, y_pred):.4f}")
    print(f"F1-score: {f1_score(y_true, y_pred):.4f}\n")

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.show()

# Evaluate Logistic Regression Model
evaluate_model(y_test, lr_preds, "Logistic Regression")

# Evaluate Random Forest Model
evaluate_model(y_test, rf_preds, "Random Forest")

import matplotlib.pyplot as plt
import numpy as np

# Model Names
model_names = ['Text + Num - LR', 'Num - LR', 'Text - LR']

# Performance Metrics for each model
accuracies = [92.21, 89.99, 87.10]  # Accuracy values
precisions = [87.23, 86.17, 87.93]  # Precision values
recalls = [90.08, 83.74, 72.00]     # Recall values
f1_scores = [88.63, 84.94, 78.71]   # F1-score values

# Plotting the comparison chart
x = np.arange(len(model_names))
width = 0.2

# Create a figure for comparison
plt.figure(figsize=(12, 7))

# Bar plots for each metric
bars_acc = plt.bar(x - 1.5*width, accuracies, width, label='Accuracy', color='cornflowerblue')
bars_prec = plt.bar(x - 0.5*width, precisions, width, label='Precision', color='darkorange')
bars_recall = plt.bar(x + 0.5*width, recalls, width, label='Recall', color='mediumseagreen')
bars_f1 = plt.bar(x + 1.5*width, f1_scores, width, label='F1-score', color='slateblue')

# Annotate each bar with the respective value
for bars, values in zip([bars_acc, bars_prec, bars_recall, bars_f1], [accuracies, precisions, recalls, f1_scores]):
    for bar, value in zip(bars, values):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f'{value:.2f}',
                 ha='center', va='bottom', fontsize=10)

# Customizing chart
plt.xticks(x, model_names, rotation=0)
plt.ylabel('Scores (%)')
plt.title('Performance Comparison of Logistic Regression Models')
plt.legend(title="Metrics", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()

# Display the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Model Names
model_names = ['Text + Num - RF', 'Text - RF', 'Num - RF']

# Performance Metrics for each model
accuracies = [88.46, 78.47, 91.22]
precisions = [91.92, 76.00, 84.27]
recalls = [72.09, 41.00, 90.91]
f1_scores = [80.80, 56.00, 87.47]

# Set bar width
width = 0.2
x = np.arange(len(model_names))

# Plotting the grouped bar chart
fig, ax = plt.subplots(figsize=(12, 7))

bars_acc = ax.bar(x - 1.5*width, accuracies, width, label='Accuracy', color='cornflowerblue')
bars_prec = ax.bar(x - 0.5*width, precisions, width, label='Precision', color='darkorange')
bars_recall = ax.bar(x + 0.5*width, recalls, width, label='Recall', color='mediumseagreen')
bars_f1 = ax.bar(x + 1.5*width, f1_scores, width, label='F1-score', color='slateblue')

# Annotating bars with their respective values
for bars, values in zip([bars_acc, bars_prec, bars_recall, bars_f1], [accuracies, precisions, recalls, f1_scores]):
    for bar, value in zip(bars, values):
        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f'{value:.2f}',
                ha='center', va='bottom', fontsize=10)

# Customizing chart
ax.set_xticks(x)
ax.set_xticklabels(model_names, rotation=0)
ax.set_ylabel('Scores (%)')
ax.set_title('Performance Comparison of Random Forest Models')
ax.legend(title="Metrics", bbox_to_anchor=(1.05, 1), loc='upper left')

plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from scipy.sparse import hstack

# Encode target variable
label_encoder = LabelEncoder()
AR['Recommended'] = label_encoder.fit_transform(AR['Recommended'])

# Selecting numerical features
num_features = ['Seat Comfort', 'Cabin Staff Service', 'Food & Beverages', 'Ground Service',
                'Inflight Entertainment', 'Wifi & Connectivity', 'Value For Money']
numerical_data = AR[num_features]
scaler = StandardScaler()
numerical_scaled = scaler.fit_transform(numerical_data)

# Text feature (Review)
text_feature = AR['Review']
tfidf = TfidfVectorizer(max_features=500)
text_transformed = tfidf.fit_transform(text_feature)

# Combining numerical and textual features
X = hstack((numerical_scaled, text_transformed))
y = AR['Recommended']

# Splitting dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train hybrid model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))